# Guide d'utilisation du projet Flask + Ollama + Redis

## ğŸš€ PrÃ©sentation du projet

Ce projet met en place une API Flask qui communique avec un serveur Ollama pour la gÃ©nÃ©ration de texte et utilise Redis pour gÃ©rer le contexte des conversations. L'ensemble est orchestrÃ© avec Docker Compose.

---

## ğŸ“¦ Structure du projet

- `app/` : Contient l'application Flask
  - `app.py` : Code principal pour les endpoints
- `Dockerfile` : Instructions pour construire l'image Flask
- `docker-compose.yml` : Configuration des services (Flask, Ollama, Redis)
- `requirements.txt` : Liste des dÃ©pendances Python
- `volume/` : Stockage persistant pour Ollama

---

## ğŸ”¥ Lancer le projet

### 1. DÃ©marrer les services

Assurez-vous d'avoir Docker et Docker Compose installÃ©s, puis lancez :

```bash
# DÃ©marre Ollama, Redis et Flask
docker-compose up --build
```

Les trois services vont dÃ©marrer et Ãªtre disponibles aux ports suivants :
- Flask API : http:////10.144.28.121:5000
- Ollama : http://localhost:11434
- Redis : localhost:6379

### 2. VÃ©rifier les logs

Vous pouvez vÃ©rifier que tout fonctionne :

```bash
docker-compose logs -f
```

---

## ğŸŒ Endpoints de l'API

### Healthcheck

VÃ©rifier que le serveur Ollama est accessible :

```http
GET /healthcheck
```

RÃ©ponse attendue :

```json
{
  "status": "OK",
  "ollama_connection": "SUCCESS"
}
```

---

### GÃ©nÃ©rer du texte

Envoyer un prompt et obtenir une rÃ©ponse du modÃ¨le Ollama :

```http
POST /generate
```

**Body JSON:**

```json
{
  "session_id": "1234",
  "prompt": "Dis-moi une blague !",
  "model": "llama3.2"
}
```

**RÃ©ponse:**

```json
{
  "model": "llama3.2",
  "response": "Pourquoi les plongeurs plongent-ils toujours en arriÃ¨re ? Parce que sinon ils tombent dans le bateau !"
}
```

---

### RÃ©cupÃ©rer le contexte d'une session

Pour voir l'historique des Ã©changes avec une session particuliÃ¨re :

```http
GET /get_context/<session_id>
```

Exemple :

```bash
curl -X GET http://localhost:5000/get_context/1234
```

**RÃ©ponse:**

```json
{
  "context": [
    "Utilisateur: Dis-moi une blague !",
    "Assistant: Pourquoi les plongeurs plongent-ils toujours en arriÃ¨re ? Parce que sinon ils tombent dans le bateau !"
  ]
}
```

---

## âš™ï¸ Gestion des conteneurs Docker

### ArrÃªter les services

```bash
docker-compose down
```

### Nettoyer les volumes (âš ï¸ supprime le contexte enregistrÃ©)

```bash
docker-compose down -v
```

### Rebuilder l'image Flask

```bash
docker-compose build flask-app
```

---

## âœ… Tests rapides

Vous pouvez tester directement depuis Postman ou avec `curl`. Assurez-vous que les services sont bien lancÃ©s et Ã©coutez les bonnes URL.

---

## ğŸ“š DÃ©pendances

Les dÃ©pendances sont dÃ©finies dans `requirements.txt` :

```plaintext
flask==2.0.1
gunicorn==21.2.0
requests==2.26.0
werkzeug==2.0.3
redis==4.5.1
```

Installez-les localement pour les tests hors Docker :

```bash
pip install -r requirements.txt
```

---

## ğŸŒŸ Remarques

- Le contexte des conversations est stockÃ© dans Redis et expire aprÃ¨s 1 heure.
- Le mode streaming est activÃ© pour Ollama pour une gÃ©nÃ©ration plus fluide.
- Les logs Flask sont configurÃ©s pour afficher les erreurs et les requÃªtes importantes.

---

Ce guide couvre les bases pour que vous puissiez dÃ©marrer, tester et comprendre le projet. N'hÃ©sitez pas si vous avez besoin de dÃ©tails sur une partie spÃ©cifique ! ğŸ’¡

