# Guide d'utilisation du projet Flask + Ollama + Redis

## üöÄ Pr√©sentation du projet

Ce projet met en place une API Flask qui communique avec un serveur Ollama pour la g√©n√©ration de texte et utilise Redis pour g√©rer le contexte des conversations. L'ensemble est orchestr√© avec Docker Compose.

---

## üì¶ Structure du projet

- `app/` : Contient l'application Flask
  - `app.py` : Code principal pour les endpoints
- `Dockerfile` : Instructions pour construire l'image Flask
- `docker-compose.yml` : Configuration des services (Flask, Ollama, Redis)
- `requirements.txt` : Liste des d√©pendances Python
- `volume/` : Stockage persistant pour Ollama

---

## üî• Lancer le projet

### 1. D√©marrer les services

Assurez-vous d'avoir Docker et Docker Compose install√©s, puis lancez :

```bash
# D√©marre Ollama, Redis et Flask
docker-compose up --build
```

Les trois services vont d√©marrer et √™tre disponibles aux ports suivants :
- Flask API : http://10.144.28.121:5000
- Ollama : http://localhost:11434
- Redis : http://localhost:6379

### 2. V√©rifier les logs

Vous pouvez v√©rifier que tout fonctionne :

```bash
docker-compose logs -f
```

---

## üåê Endpoints de l'API

### Healthcheck

V√©rifier que le serveur Ollama est accessible :

```http
GET /healthcheck
```

R√©ponse attendue :

```json
{
  "status": "OK",
  "ollama_connection": "SUCCESS"
}
```

---

### G√©n√©rer du texte

Envoyer un prompt et obtenir une r√©ponse du mod√®le Ollama :

```http
POST /generate
```

**Body JSON:**

```json
{
  "session_id": "test_session",
  "prompt": "Dis-moi une blague !",
  "model": "llama3.2"
}
```

**R√©ponse:**

```json
{
  "model": "llama3.2",
  "response": "Pourquoi les plongeurs plongent-ils toujours en arri√®re ? Parce que sinon ils tombent dans le bateau !"
}
```

---

### R√©cup√©rer le contexte d'une session

Pour voir l'historique des √©changes avec une session particuli√®re :

```http
GET /get_context/<session_id>
```

Exemple :

```bash
curl -X GET http://10.144.28.121:5000/get_context/test_session
```

**R√©ponse:**

```json
{
  "context": [
    "Utilisateur: Dis-moi une blague !",
    "Assistant: Pourquoi les plongeurs plongent-ils toujours en arri√®re ? Parce que sinon ils tombent dans le bateau !"
  ]
}
```

---

## ‚öôÔ∏è Gestion des conteneurs Docker

### Arr√™ter les services

```bash
docker-compose down
```

### Nettoyer les volumes (‚ö†Ô∏è supprime le contexte enregistr√©)

```bash
docker-compose down -v
```

### Rebuilder l'image Flask

```bash
docker-compose build flask-app
```

---

## ‚úÖ Tests rapides

Vous pouvez tester directement depuis Postman ou avec `curl`. Assurez-vous que les services sont bien lanc√©s et √©coutez les bonnes URL.

```bash
curl -X POST http://10.144.28.121:5000/generate -H "Content-Type: application/json" -d '{"session_id": "test", "prompt": "salut"}'
```
---

## üìö D√©pendances

Les d√©pendances sont d√©finies dans `requirements.txt` :

```plaintext
flask==2.0.1
gunicorn==21.2.0
requests==2.26.0
werkzeug==2.0.3
redis==4.5.1
```

Installez-les localement pour les tests hors Docker :

```bash
pip install -r requirements.txt
```

---

## üåü Remarques

- Le contexte des conversations est stock√© dans Redis et expire apr√®s 1 heure.
- Le mode streaming est activ√© pour Ollama pour une g√©n√©ration plus fluide.
- Les logs Flask sont configur√©s pour afficher les erreurs et les requ√™tes importantes.

---